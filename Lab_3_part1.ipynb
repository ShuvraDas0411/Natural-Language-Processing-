{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b15d29a-094c-4d46-9ba0-68ae32f4c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Load IMDb dataset\n",
    "file_path = '/Users/shuvradas/Downloads/IMDB Dataset.csv'  # Update this path with your dataset location\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess text (remove HTML tags, punctuation, lowercase)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Convert sentiment labels to binary (positive=1, negative=0)\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ce38f0-90e8-4501-882a-005915c683f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "embedding_dim = 100\n",
    "glove_path = '/Users/shuvradas/Downloads/glove.6B/glove.6B.100d.txt'  # Update this path with your GloVe embeddings location\n",
    "embedding_index = {}\n",
    "\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector\n",
    "\n",
    "# Tokenize the text\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert texts to sequences and pad them\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = 100  # Define max length for padding\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if word in embedding_index:\n",
    "        embedding_matrix[idx] = embedding_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceaca6a9-d094-46b9-9724-c5bd68bad7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = torch.tensor(self.reviews[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return review, label\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = IMDBDataset(X_train_pad, y_train.values)\n",
    "test_dataset = IMDBDataset(X_test_pad, y_test.values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff171bd-f9d9-4ca7-957d-a1996e27e115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6833\n",
      "Epoch 2/5, Loss: 0.6627\n",
      "Epoch 3/5, Loss: 0.6526\n",
      "Epoch 4/5, Loss: 0.6695\n",
      "Epoch 5/5, Loss: 0.6548\n"
     ]
    }
   ],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_dim).to(x.device)  # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Use the output of the last time step\n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model_rnn = VanillaRNN(vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model_rnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for reviews, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(reviews)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Train the Vanilla RNN model\n",
    "train_model(model_rnn, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51927200-db0e-49f3-8735-20468d5b93ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6093\n",
      "Epoch 2/5, Loss: 0.4513\n",
      "Epoch 3/5, Loss: 0.3863\n",
      "Epoch 4/5, Loss: 0.3500\n",
      "Epoch 5/5, Loss: 0.3233\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_dim).to(x.device)  # Initial hidden state\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_dim).to(x.device)  # Initial cell state\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Use the output of the last time step\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model_lstm = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "# Train the LSTM model\n",
    "train_model(model_lstm, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf7509c-80f1-4a9f-96b7-df5a387865ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for reviews, labels in test_loader:\n",
    "            outputs = model(reviews)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Convert logits to probabilities and then to binary predictions\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "745dd1f3-e658-4b81-88f4-2690d6eae2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6728\n",
      "Epoch 2/5, Loss: 0.6308\n",
      "Epoch 3/5, Loss: 0.6044\n",
      "Epoch 4/5, Loss: 0.5641\n",
      "Epoch 5/5, Loss: 0.5380\n",
      "Vanilla RNN with On-the-Fly Embeddings - Loss: 0.6103, Accuracy: 0.6732\n"
     ]
    }
   ],
   "source": [
    "class VanillaRNNOnTheFly(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(VanillaRNNOnTheFly, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_dim).to(x.device)  # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Use the output of the last time step\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model_rnn_otf = VanillaRNNOnTheFly(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model_rnn_otf.parameters(), lr=0.001)\n",
    "\n",
    "# Train the Vanilla RNN model with on-the-fly embeddings\n",
    "train_model(model_rnn_otf, train_loader, criterion, optimizer)\n",
    "\n",
    "# Evaluate the Vanilla RNN model with on-the-fly embeddings\n",
    "rnn_otf_loss, rnn_otf_accuracy = evaluate_model(model_rnn_otf, test_loader, criterion)\n",
    "print(f\"Vanilla RNN with On-the-Fly Embeddings - Loss: {rnn_otf_loss:.4f}, Accuracy: {rnn_otf_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fdbaa91-ebdb-42cc-a356-f2d2a2e321ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.5994\n",
      "Epoch 2/5, Loss: 0.3961\n",
      "Epoch 3/5, Loss: 0.2665\n",
      "Epoch 4/5, Loss: 0.1857\n",
      "Epoch 5/5, Loss: 0.1203\n",
      "LSTM with On-the-Fly Embeddings - Loss: 0.4113, Accuracy: 0.8534\n"
     ]
    }
   ],
   "source": [
    "class LSTMModelOnTheFly(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModelOnTheFly, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_dim).to(x.device)  # Initial hidden state\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_dim).to(x.device)  # Initial cell state\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Use the output of the last time step\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model_lstm_otf = LSTMModelOnTheFly(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model_lstm_otf.parameters(), lr=0.001)\n",
    "\n",
    "# Train the LSTM model with on-the-fly embeddings\n",
    "train_model(model_lstm_otf, train_loader, criterion, optimizer)\n",
    "\n",
    "# Evaluate the LSTM model with on-the-fly embeddings\n",
    "lstm_otf_loss, lstm_otf_accuracy = evaluate_model(model_lstm_otf, test_loader, criterion)\n",
    "print(f\"LSTM with On-the-Fly Embeddings - Loss: {lstm_otf_loss:.4f}, Accuracy: {lstm_otf_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10c3a283-8dea-4710-b4ed-1fa2468402dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN with GloVe Embeddings - Loss: 0.7688, Accuracy: 0.5328\n",
      "LSTM with GloVe Embeddings - Loss: 0.3334, Accuracy: 0.8539\n",
      "Vanilla RNN with On-the-Fly Embeddings - Loss: 0.6103, Accuracy: 0.6732\n",
      "LSTM with On-the-Fly Embeddings - Loss: 0.4113, Accuracy: 0.8534\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Vanilla RNN model with GloVe embeddings\n",
    "rnn_loss, rnn_accuracy = evaluate_model(model_rnn, test_loader, criterion)\n",
    "print(f\"Vanilla RNN with GloVe Embeddings - Loss: {rnn_loss:.4f}, Accuracy: {rnn_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the LSTM model with GloVe embeddings\n",
    "lstm_loss, lstm_accuracy = evaluate_model(model_lstm, test_loader, criterion)\n",
    "print(f\"LSTM with GloVe Embeddings - Loss: {lstm_loss:.4f}, Accuracy: {lstm_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the Vanilla RNN model with on-the-fly embeddings\n",
    "rnn_otf_loss, rnn_otf_accuracy = evaluate_model(model_rnn_otf, test_loader, criterion)\n",
    "print(f\"Vanilla RNN with On-the-Fly Embeddings - Loss: {rnn_otf_loss:.4f}, Accuracy: {rnn_otf_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the LSTM model with on-the-fly embeddings\n",
    "lstm_otf_loss, lstm_otf_accuracy = evaluate_model(model_lstm_otf, test_loader, criterion)\n",
    "print(f\"LSTM with On-the-Fly Embeddings - Loss: {lstm_otf_loss:.4f}, Accuracy: {lstm_otf_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d34ef5-4c42-4e7e-8136-07e38f8b7b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
